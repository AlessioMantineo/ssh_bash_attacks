{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b><font size=6>Machine Learning for Networks<b><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<left><b><font size=4>SSH Shell Attack session<b><left>                                                                   \n",
    "##### Group 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.dates as mdates\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Models \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<left><b><font size=4>Section 1 â€“ Data exploration and pre-processing<b><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset with all the SSH sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "df_original = pd.read_parquet('ssh_attacks.parquet')\n",
    "df=df_original.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. When are the attacks performed? Analyze the temporal series.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'first_timestamp' column to datetime objects\n",
    "df['first_timestamp'] = pd.to_datetime(df['first_timestamp'])\n",
    "\n",
    "# Creating a new DataFrame containing only attack instances\n",
    "# Excluding sessions labeled as \"Harmless\" with a single label\n",
    "df_attacks = df.loc[~df[\"Set_Fingerprint\"].apply(lambda x : \"Harmless\" in x and len(x) == 1)]\n",
    "\n",
    "# Count the number of attacks per day\n",
    "# Extract the date from 'first_timestamp', count occurrences, sort by date\n",
    "attacks_per_day = df_attacks['first_timestamp'].dt.date.value_counts().sort_index().to_frame(\"Number_of_attacks_per_day\")\n",
    "\n",
    "# Display the resulting DataFrame showing the number of attacks per day\n",
    "attacks_per_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis of attack occurrences based on the temporal series of first_timestamp showcases a trend in attack frequency over time.\n",
    "The dataset has been transformed to interpret the first_timestamp column as datetime objects for accurate temporal analysis. The subsequent process isolates attack instances within the dataset, excluding records tagged as \"Harmless\" with a single label.\n",
    "The resulting analysis presents the number of attacks per day:\n",
    "\n",
    "| Date_time | Attacks |\n",
    "| --- | --- |\n",
    "|June 4th, 2019 | 82 attacks |\n",
    "|June 5th, 2019 | 124 attacks |\n",
    "|June 6th, 2019 | 117 attacks |\n",
    "|June 7th, 2019 | 121 attacks |\n",
    "|June 8th, 2019 | 118 attacks |\n",
    "| ... (continues with dates up to) |\n",
    "| February 25th, 2020| 649 attacks |\n",
    "| February 26th, 2020| 483 attacks |\n",
    "| February 27th, 2020| 551 attacks |\n",
    "| February 28th, 2020| 580 attacks |\n",
    "| February 29th, 2020| 627 attacks |\n",
    "\n",
    "This temporal series reveals fluctuations in attack intensity over time, with notable spikes and drops in attack occurrences. The observations suggest potential patterns or trends that could be further explored to understand the dynamics of these SSH shell attacks across different periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size for the plot\n",
    "plt.figure(figsize=(9,5))\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Create a line plot using seaborn\n",
    "sns.lineplot(\n",
    "    data = attacks_per_day,          # Use the 'attacks_per_day' DataFrame for plotting\n",
    "    x = attacks_per_day.index,       # X-axis represents the timestamp of attacks\n",
    "    y = \"Number_of_attacks_per_day\", # Y-axis represents the number of attacks per day\n",
    "    marker='o',                      # Marker style for data points\n",
    "    linestyle='-',                   # Style of the line connecting the data points\n",
    "    color= 'blue',                   # Color of the line\n",
    "    markersize=5                     # Size of markers\n",
    ")\n",
    "\n",
    "# Set plot title and labels for axes\n",
    "plt.title('Number of Attacks')\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of attacks\")\n",
    "\n",
    "# Format the date on the x-axis to display day-month-year\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d-%m-%y'))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Ensure proper layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization depicting attack frequencies over time reveals distinct patterns:\n",
    "\n",
    "Between June 2019 and September 2019, there is a conspicuous decrease in attack occurrences, indicating a phase of minimal activity. Subsequently, there is a significant surge in attack instances towards the latter part of 2019, signifying a notable rise in both the frequency and intensity of attacks during this period.\n",
    "\n",
    "This timeline underscores a stark contrast between the relatively quiet phase observed from June to September 2019 and the pronounced escalation in attack activities, particularly notable in the latter months of the year. This shift in trend emphasizes a substantial alteration in attack behavior, marked by an extended period of low activity succeeded by a considerable surge in attack incidents towards the year's end.\n",
    "\n",
    "Moreover, at the beginning of 2020, there is a noticeable decline in attack occurrences once more. This decline follows the heightened activity observed in late 2019, representing a shift from the increased attack rates back to a decreased frequency as the year transitions into its initial months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code aggregates attack instances per year, creating a DataFrame named attacks_per_year\n",
    "\n",
    "# Group attack instances by year and count occurrences\n",
    "attacks_per_year = df_attacks.groupby(df['first_timestamp'].dt.year).size().to_frame(\"Number_attacks\").reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "attacks_per_year.rename(columns={\"first_timestamp\": \"Year\"}, inplace=True)\n",
    "attacks_per_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contrast in attack counts between the two years, with 2019 showing a significantly higher number of attacks compared to 2020, can be reasonably attributed to the limited temporal coverage of the dataset for the year 2020. With data available for only two months of 2020, the reduced number of observations in this period is expected and explains the lower count of attacks for that year compared to the extensive records available for 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size for the plot\n",
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "# Set the figure size for the plot\n",
    "sns.barplot(\n",
    "    data = attacks_per_year,   # Use the 'attacks_per_year' DataFrame for plotting\n",
    "    x = \"Year\",                # X-axis represents the years (2019 and 2020)\n",
    "    y = \"Number_attacks\",      # Y-axis represents the number of attacks\n",
    "    hue= \"Year\",\n",
    "    palette = ['blue','orange']\n",
    ")\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.ylabel(\"Number of attacks\")\n",
    "plt.title(\"Number of total attacks in 2019 and 2020\")\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter attack instances for the year 2019\n",
    "df_attacks_2019 = df_attacks.loc[df_attacks[\"first_timestamp\"].dt.year == 2019]\n",
    "# Group attacks by month and count occurrences\n",
    "df_attacks_2019_month = df_attacks_2019.groupby(df_attacks_2019[\"first_timestamp\"].dt.month).size().to_frame(\"Number_attacks_2019_month\").reset_index()\n",
    "\n",
    "# Filter attack instances for the year 2020\n",
    "df_attacks_2020 = df_attacks.loc[df_attacks[\"first_timestamp\"].dt.year == 2020]\n",
    "# Group attacks by month and count occurrences\n",
    "df_attacks_2020_month = df_attacks_2020.groupby(df_attacks_2020[\"first_timestamp\"].dt.month).size().to_frame(\"Number_attacks_2020_month\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a figure with two subplots\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Plot for 2019\n",
    "plt.subplot(1,2,1)\n",
    "sns.barplot(\n",
    "    data = df_attacks_2019_month,\n",
    "    x = \"first_timestamp\",\n",
    "    y = \"Number_attacks_2019_month\",\n",
    "    color = 'blue'  \n",
    ")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Number of attacks\")\n",
    "plt.title(\"2019\")\n",
    "\n",
    "# Plot for 2020\n",
    "plt.subplot(1,2,2)\n",
    "sns.barplot(\n",
    "    data = df_attacks_2020_month,\n",
    "    x = \"first_timestamp\",\n",
    "    y = \"Number_attacks_2020_month\",\n",
    "    color = 'orange'\n",
    ")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Number of attacks\")\n",
    "plt.title(\"2020\")\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout(pad = 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar charts reveal intriguing trends:\n",
    "\n",
    "- In 2019, there is a noticeable surge in the number of attacks from months 9 to 12. A progressive increase in attack activity is observed during these months, reaching a peak towards the year's end.\n",
    "\n",
    "- At the onset of 2020, in months 1 and 2, a relatively similar frequency of attacks is noted, approximately representing half the number of attacks compared to month 10 in 2019.\n",
    "\n",
    "These patterns outline a significant uptick in attack activity towards the end of 2019, followed by a comparatively steady beginning in 2020 with a considerably lower number of attacks compared to the peak period of the previous year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering harmless sessions based on 'Set_Fingerprint' column\n",
    "df_harmless = df[df[\"Set_Fingerprint\"].apply(lambda x : \"Harmless\" in x and len(x) == 1)]\n",
    "\n",
    "# Counting harmless sessions per day\n",
    "harmless_per_day = df_harmless['first_timestamp'].dt.date.value_counts().sort_index().to_frame(\"Number_harmless_per_day\")\n",
    "\n",
    "# Creating a bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.barplot(\n",
    "    data = harmless_per_day,\n",
    "    x = harmless_per_day.index, \n",
    "    y = \"Number_harmless_per_day\",\n",
    "    hue = harmless_per_day.index,\n",
    "    legend = False,\n",
    "    palette = \"icefire\"\n",
    ")\n",
    "plt.title('Temporal Distribution of Harmless Sessions')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Harmless Sessions')\n",
    "plt.xticks(rotation = 45, fontsize = 4)  # Rotate x-axis labels by 45 degrees\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2. Extract features from the attack sessions. How does the empirical distribution of the number of\n",
    "characters in each session look like? How is the distribution of the number of word per session?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add number of characters and words as features\n",
    "# Number_characters: Calculates the number of alphanumeric characters in each session.\n",
    "df['number_characters'] = df['full_session'].apply(lambda x: len([char for char in x if char.isalpha()]))\n",
    "\n",
    "# Number_words: Computes the number of words (considering only alphanumeric characters) in each session.\n",
    "df['number_words'] = df['full_session'].apply(lambda x: len([char for char in x.split() if char.isalpha()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code creates two side-by-side histograms:\n",
    "# Histogram of Character Counts per Session: Displays the distribution of the number of characters in each session. It bins the data into 50 bins.\n",
    "# Histogram of Word Counts per Session: Shows the distribution of the number of words in each session. It also bins the data into 50 bins.\n",
    "# These histograms help visualize the distributions of character and word counts within the attack sessions, providing insights into the length and complexity of these sessions in terms of characters and words. \n",
    "\n",
    "# Tracking histograms for character and word counts per session.\n",
    "plt.figure(figsize=(11, 5))\n",
    "\n",
    "# Histogram for the number of characters per session.\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.hist(df['number_characters'], bins=50, color = 'blue')\n",
    "plt.title('Distribution of the number of characters per session')\n",
    "plt.xlabel('Number of Characters')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "\n",
    "# Histogram for the number of words per session\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df['number_words'], bins = 50 , color = 'orange') \n",
    "plt.title('Distribution of the number of words per session')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot, it's possible to see that the distribution of words per session and the distribution of characters per session predominantly concentrate below 2000 words and below 20,000 characters, respectively.\n",
    "\n",
    "To have a clearer idea of the distribution, we decided to limit the x-axis for both plots so that we can closely examine these two distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking histograms for character and word counts per session.\n",
    "plt.figure(figsize=(11, 5))\n",
    "\n",
    "# Histogram for the number of characters per session.\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.hist(df['number_characters'], bins=50, range=(0, 700), color = 'blue') #\n",
    "plt.title('Distribution of the number of characters per session')\n",
    "plt.xlabel('Number of Characters')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0, 700)\n",
    "\n",
    "# Histogram for the number of words per session\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df['number_words'], bins = 50, range = (0, 70) , color = 'orange') #\n",
    "plt.title('Distribution of the number of words per session')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0, 70)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histograms, it is observed that the distribution of character counts per session centers is concentrated around approximately 350 characters.\n",
    "Moreover, regarding the number of words per session, the distribution frequently peaks around 40 to 48 words. This indicates that sessions often contain this range of word counts, emphasizing a typical occurrence of sessions with this word count range. These insights provide a clear understanding of the common lengths observed within the attack sessions, both in terms of characters and words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3. What are the most common words in the sessions?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the 'full_session' column from the DataFrame\n",
    "df_words = df['full_session']\n",
    "\n",
    "# Joining all text from 'full_session' into a single string\n",
    "all_text = ' '.join(df['full_session'])\n",
    "\n",
    "# Splitting the text into individual words\n",
    "all_words = all_text.split()\n",
    "\n",
    "# Cleaning the text, leaving only alpha numeric words\n",
    "cleaning_text = [all_words[i] for i in range(len(all_words)) if all_words[i].isalpha()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each word and extract the top 10 most common words\n",
    "df_list = pd.Series(cleaning_text).value_counts().head(10)\n",
    "\n",
    "# Identify the most common word and its frequency\n",
    "most_common_word = df_list.idxmax()    # Most common word\n",
    "frequency = df_list.max()              # Frequency of the most common word\n",
    "\n",
    "# Convert the Series of word frequencies to a dictionary\n",
    "word_freq=df_list.to_dict()\n",
    "most_common_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common word is : 'grep'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary that holds the frequencies of the top 10 most common words\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "\n",
    "# Display the WordCloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This representation emphasizes words based on their frequency in the sessions. Words that appear more frequently will be displayed larger and more prominently within the WordCloud. The interpolation='bilinear' argument enhances the image quality for better clarity. The plt.axis('off') command removes the axis for a cleaner visual appearance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4. How are the intents distributed? How many intents per session do you observe? What are the most common intents? How are the intents distributed in time?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting relevant columns: session_id and Set_Fingerprint\n",
    "intents_df = df[[\"session_id\", \"Set_Fingerprint\"]]\n",
    "\n",
    "# Expanding the 'Set_Fingerprint' column to individual intents and sessions\n",
    "intents_df_exploted = intents_df.explode('Set_Fingerprint')\n",
    "\n",
    "# Grouping by session and counting the number of intents per session\n",
    "intents_df_grouped = intents_df_exploted.groupby(\"session_id\").size()\n",
    "\n",
    "# Plotting the distribution of intents per session\n",
    "intents_df_grouped.plot(\n",
    "    figsize = (10,5), \n",
    "    xlabel = \"Sessions\", \n",
    "    ylabel = \"Number of Intents\", \n",
    "    title = \"Number of Intents per Sessions\",\n",
    "    color = 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = intents_df_exploted.groupby('Set_Fingerprint').count().sort_values(by='session_id', ascending=False)\n",
    "\n",
    "# Distribution plot of intents \n",
    "# Set up Seaborn style\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# Generate the figures\n",
    "plt.figure(figsize=(6,3))\n",
    "sns.barplot(intents, x='session_id', y=intents.index, color='blue', hue_order=intents.index)\n",
    "plt.title('Distribution of Intents')\n",
    "plt.xlabel('Number of intents')\n",
    "plt.ylabel('Type of Intents')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The presented bar chart shows the most common intentions found in the dataset; Discovery, Persistence and Execution lead the most used type of attacks for each of the sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the 'Set_Fingerprint' column and group by timestamp and intents, filling missing values with 0\n",
    "df_grouped = df.explode('Set_Fingerprint').groupby([pd.Grouper(key='first_timestamp', freq='D'), 'Set_Fingerprint']).size().unstack().fillna(0)\n",
    "\n",
    "# Plotting the distribution of intents over time\n",
    "df_grouped.plot(\n",
    "    figsize = (10,6), \n",
    "    xlabel = \"Date\", \n",
    "    ylabel = \"Distribution of the Intents\", \n",
    "    title = \"Distribution of the Intents in Time\")\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d-%m-%y'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of intents unfolds as follows:\n",
    "\n",
    "**Defense Evasion**:\n",
    "No significant peaks noticed, with a few attacks observed between July and September 2019, remaining consistently low alongside 'Harmless', 'Impact', and 'Other' intents.\n",
    "\n",
    "**Execution**:\n",
    "Displays a sharp peak towards the end of 2019, notably in the last two months.\n",
    "\n",
    "**Persistence and Discovery**:\n",
    "Showcase an intriguing trend, reaching their highest peaks towards the end of 2019. These intents exhibit the highest frequency, notably surging towards the year-end, reaching maximum levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5. How can text represented numerically? Try to convert the text into numerical representations\n",
    "(vectors) through Bag of Words (BoW)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set()\n",
    "df['full_session'].apply(lambda x: [stop_words.add(character) for character in x if not character.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words = list(stop_words), min_df = 0.05, max_df = 0.95)\n",
    "bow = count_vectorizer.fit_transform(df[\"full_session\"])\n",
    "df[\"bag_of_words\"] = pd.Series(bow.toarray().tolist())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.6. Associate each word in each attack session with its TF-IDF value (Term Frequency-Inverse Document Frequency)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words=list(stop_words), min_df=0.05, max_df=0.95)\n",
    "tfid = tfidf_vectorizer.fit_transform(df[\"full_session\"])\n",
    "df[\"tfid\"] = pd.Series(tfid.toarray().tolist())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(tfid.toarray(), index=df[\"full_session\"].index, columns = list(tfidf_vectorizer.vocabulary_.keys()))\n",
    "# Concatenate along columns (axis=1)\n",
    "result_df = pd.concat([df, tfidf_df], axis=1)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "df_class = result_df.copy()\n",
    "df_features = df_class.drop(columns=[\"session_id\", \"full_session\", \"first_timestamp\", \"Set_Fingerprint\", \"number_characters\", \"number_words\", \"bag_of_words\", \"tfid\"])\n",
    "correlation_matrix = df_features.corr().abs()\n",
    "\n",
    "# Compute the heatmap of the correlation matrix\n",
    "plt.figure(figsize=(50,50))\n",
    "sns.heatmap(correlation_matrix, cmap='Blues', annot=True, vmin=.0, vmax=1, cbar_kws={'label':'Correlation'})\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Correlation matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features having a correlation (so with a covariance) > 0.98\n",
    "c = correlation_matrix[correlation_matrix > 0.98]\n",
    "s = c.unstack()\n",
    "so = s.sort_values(ascending=False).reset_index()\n",
    "\n",
    "# Get strongly correlatead features, removing pairs having correlation = 1 because of the diagonal, i.e., correlation between one feature and itself\"\n",
    "so = so[(so[0].isnull()==False) & (so[\"level_0\"] != so[\"level_1\"])]\n",
    "to_be_deleted = []\n",
    "candidates = list(so[\"level_0\"])\n",
    "\n",
    "# Get the unique set of features to be deleted\n",
    "# Notice that we discard one feature per time considering the case where a feature is strongly correlated with multiple features\n",
    "subset_so = so\n",
    "for candidate in candidates:\n",
    "    if (candidate in list(subset_so[\"level_0\"])): \n",
    "        to_be_deleted.append(candidate) # add the feature to the removed candidates\"\n",
    "        subset_so = subset_so[(subset_so[\"level_0\"] != candidate) & (subset_so[\"level_1\"] != candidate)] # remove the rows that the removed feature is involved\"\n",
    "print(len(to_be_deleted), 'features to be removed')\n",
    "to_be_deleted\n",
    "df_features = df_features.drop(columns=to_be_deleted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_corr = pd.concat([result_df[[\"session_id\", \"full_session\",\"first_timestamp\",\"Set_Fingerprint\",\"number_characters\",\"number_words\",\"bag_of_words\",\"tfid\"]], df_features], axis=1)\n",
    "result_df_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<left><b><font size=4>Section 2 â€“ Supervised Learning â€“ Classification<b><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classify the tactics of an attack session, based on the used words in the text and also possibly on time. Notice that each session have multiple labels. Hence you can decompose the problem into multiple binary classification problems. For each attack session, you have to solve the 7 binary classification problem, one for each possible label {'Persistence', 'Discovery', 'Defense Evasion', 'Execution', 'Impact', 'Other', 'Harmless'}.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class = result_df_corr.copy()\n",
    "print(df_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Perform a split to segment the dataset into training and test dataset. If you want to standardize your dataset, fit the scaler on training set and transforming both training and test. Notice that the sklearn implementation of tf-idf already performs the standardization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = df_features.columns\n",
    "\n",
    "X_feature = result_df_corr.filter(features_names)\n",
    "y_feature =  result_df_corr[\"Set_Fingerprint\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_feature,\n",
    "    y_feature,\n",
    "    train_size = 0.7,             # 70% of the data is for trainning\n",
    "    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The size of trainning set is:', len(X_train))\n",
    "print('The size of test set is:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standardization of the Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the TF-IDF pre-processing was applied previously to all sessions, the data considered as features are already standardized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standardization of the Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Use MultiLabelBinarizer to transform the labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train_mlb = mlb.fit_transform(y_train)\n",
    "y_test_mlb = mlb.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing Techniques** \n",
    "<br>\n",
    "<div style=\"text-align: justify\"> A <b>MultiLabelBinarizer</b> is a transformer that is used for multi-label classification problems, in order to handle the cases where each sample belongs to multiple classes simultaneously. The purpose of MultiLabelBinarizer is to convert a collection of sequences of labels into a binary matrix format. The binary classification of each label in the 'Set_Fingerprint' column was performed by converting the multi-class label matrix into a binary matrix, where each column represents one of the possible classes and each row represents one instance. </div><br>\n",
    "\n",
    "<div style=\"text-align: justify\"> <b>TF-IDF </b> (explain technique here) </div><br>\n",
    "\n",
    "<div style=\"text-align: justify\"> MultiLabelBinarizer is used to handle categorical variables before fitting a model, as most machine learning algorithms can only handle numerical data.</div><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Choose at least 2 ML methods, and perform the model training, with default parameter\n",
    "configuration, evaluating the performance on both training and test set. Output the confusion\n",
    "matrix and classification report. Do you observe overfitting or under-fitting? Which model\n",
    "generates the best performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<left><b><font size=4> First ML Method <b><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest (RF)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestClassifier(n_estimators=30, max_depth=15)\n",
    "# Trainning the model\n",
    "model_rf.fit(X_train, y_train_mlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predictions on training set\n",
    "y_train_pred = model_rf.predict(X_train)\n",
    "\n",
    "# Predictions on test set\n",
    "y_test_pred = model_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Evaluate performance on training set\n",
    "report_trainning = classification_report(y_train_mlb.argmax(axis=1), y_train_pred.argmax(axis=1), output_dict=True)\n",
    "df_report_trainning = pd.DataFrame(report_trainning).transpose()\n",
    "df_report_trainning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation Set (Test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance on test set\n",
    "report_test = classification_report(y_test_mlb.argmax(axis=1), y_test_pred.argmax(axis=1), output_dict=True)\n",
    "df_report_test = pd.DataFrame(report_test).transpose()\n",
    "df_report_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Set:\")\n",
    "print(confusion_matrix(y_train_mlb.argmax(axis=1), y_train_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation Set:\")\n",
    "print(confusion_matrix(y_test_mlb.argmax(axis=1), y_test_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write comments here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<left><b><font size=4>Second ML Method<b><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Nearest Neighbors (KNN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_mlb\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the k-NN model\n",
    "knn = KNeighborsClassifier()\n",
    "# Train the model on the training data\n",
    "knn.fit(X_train, y_train_mlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the test data\n",
    "accuracy = knn.score(X_test, y_test_mlb)\n",
    "print(f\"Accuracy of the k-NN model: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test set\n",
    "predictions = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will print a series of confusion matrices, one for each label, displaying true positive (top-left), false negative (bottom-left), false positive (top-right), and true negative (bottom-right) counts.\n",
    "- True Positives (TP): Predicted correctly as positive.\n",
    "- False Positives (FP): Predicted as positive but actually negative.\n",
    "- False Negatives (FN): Predicted as negative but actually positive.\n",
    "- True Negatives (TN): Predicted correctly as negative.\n",
    "\n",
    "Each value in the confusion matrix represents the count of instances falling into these categories for a specific label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with a confusion matrix and classification report\n",
    "\n",
    "\n",
    "confusion = multilabel_confusion_matrix(y_test_mlb, predictions)\n",
    "# Printing the confusion matrix\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    print(f\"Confusion Matrix for {label}:\")\n",
    "    print(confusion[i])\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.title(f'Confusion Matrix for {label}')\n",
    "    plt.imshow(confusion[i], cmap='Blues', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.xticks(np.arange(2), ['Negative', 'Positive'])\n",
    "    plt.yticks(np.arange(2), ['Negative', 'Positive'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report provides a comprehensive overview of key classification metrics for each label. It includes metrics such as precision, recall, F1-score, and support.\n",
    "- Precision:\n",
    "  -  It measures the accuracy of the positive predictions. Precision is the ratio of correctly predicted positive observations to the total predicted positives. It's calculated as TP / (TP + FP).\n",
    "- Recall (Sensitivity or True Positive Rate):\n",
    "  -  It measures the proportion of actual positives that were correctly predicted. Recall is the ratio of correctly predicted positive observations to the all observations in the actual class. It's calculated as TP / (TP + FN).\n",
    "- F1-score:\n",
    "  -  It's the harmonic mean of precision and recall. It provides a balance between precision and recall. It's calculated as 2 * (precision * recall) / (precision + recall).\n",
    "- Support:\n",
    "  - It's the number of actual occurrences of the class in the specified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test_mlb, predictions, target_names=mlb.classes_, output_dict=True)\n",
    "df_report= pd.DataFrame(report).transpose()\n",
    "print(\"Classification Report:\")\n",
    "df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report of Random Forest (RF):\")\n",
    "display(df_report_trainning)\n",
    "\n",
    "\n",
    "print(\"Classification Report of k-NN model:\")\n",
    "display(df_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Tune the hyper-parameters of the models through cross-validation. How do performance vary?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRID SEARCH\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "knn_pipe = Pipeline([('mms', MinMaxScaler()),\n",
    "                     ('knn', KNeighborsClassifier())])\n",
    "\n",
    "params = [{'knn__n_neighbors': [3, 5, 7, 9],\n",
    "         'knn__weights': ['uniform', 'distance'],\n",
    "         'knn__leaf_size': [15, 20]}]\n",
    "\n",
    "gs_knn = GridSearchCV(knn_pipe,\n",
    "                      param_grid=params,\n",
    "                      scoring='f1_macro',\n",
    "                      cv=5)\n",
    "\n",
    "gs_knn.fit(X_train, y_train_mlb)\n",
    "gs_knn.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the f1 macro reached for each combination\n",
    "y = gs_knn.cv_results_[\"mean_test_score\"].tolist()\n",
    "x = [i for i in range (1, len(y)+1)]\n",
    "mean_test_score_df = pd.DataFrame()\n",
    "mean_test_score_df[\"f1_macro\"] = y\n",
    "print(mean_test_score_df)\n",
    "\n",
    "sns.barplot(\n",
    "    x = mean_test_score_df.index, \n",
    "    y = \"f1_macro\",\n",
    "    data = mean_test_score_df\n",
    ")\n",
    "\n",
    "# Add a title and labels to the plot\n",
    "plt.title('F1-macro Scores for Different Parameters')\n",
    "plt.xlabel('Combination')\n",
    "plt.ylabel('F1-macro Score')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "#fig, ax = plt.subplots(figsize=(10, 5))\n",
    "#ax.bar(x, y)\n",
    "#ax.grid()\n",
    "#gs_knn.cv_results_[\"mean_test_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the k-NN model\n",
    "knn = KNeighborsClassifier( leaf_size = 15, n_neighbors = 3, weights = 'distance')\n",
    "# Train the model on the training data\n",
    "knn.fit(X_train, y_train_mlb)\n",
    "# Evaluate the model's performance on the test data\n",
    "accuracy = knn.score(X_test, y_test_mlb)\n",
    "print(f\"Accuracy of the k-NN model: {accuracy:.2f}\")\n",
    "# Generate predictions on the test set\n",
    "predictions = knn.predict(X_test)\n",
    "\n",
    "report = classification_report(y_test_mlb, predictions, target_names=mlb.classes_, output_dict=True)\n",
    "df_report= pd.DataFrame(report).transpose()\n",
    "print(\"Classification Report:\")\n",
    "df_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Comments on the results for each on the intents.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5 Explore the possible features: try combining features differently, e.g., does tf-idf improve or worsen\n",
    "performance? Think about the problem and summarize the ways you have tried (even those that\n",
    "did not work).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<left><b><font size=4>Section 3 â€“ Unsupervised Learning â€“ Clustering<b><left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
