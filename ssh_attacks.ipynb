{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b><font size=6>Machine Learning for Networks<b><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<left><b><font size=4>SSH Shell Attack session<b><left>                                                                   \n",
    "##### Group 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.dates as mdates\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# Models \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report\n",
    "\n",
    "#Language Models Exploration \n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense  \n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, rand_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<left><b><font size=4>Section 1 â€“ Data exploration and pre-processing<b><left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_session(full_session):\n",
    "    return [word for word in list(filter(None, re.split(\";|/|-|\\||\\.|=|$| \", full_session))) if word.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset with all the SSH sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_parquet('ssh_attacks.parquet')\n",
    "df=df_original.copy()\n",
    "df['first_timestamp'] = pd.to_datetime(df['first_timestamp'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. When are the attacks performed? Analyze the temporal series.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new DataFrame containing only attack instances\n",
    "# Excluding sessions labeled as \"Harmless\" with a single label\n",
    "df_attacks = df.loc[~df[\"Set_Fingerprint\"].apply(lambda x : \"Harmless\" in x and len(x) == 1)]\n",
    "attacks_per_day = df_attacks['first_timestamp'].dt.date.value_counts().sort_index().to_frame(\"Number_of_attacks_per_day\")\n",
    "attacks_per_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis of attack occurrences based on the temporal series of first_timestamp showcases a trend in attack frequency over time.\n",
    "The dataset has been transformed to interpret the first_timestamp column as datetime objects for accurate temporal analysis. The subsequent process isolates attack instances within the dataset, excluding records tagged as \"Harmless\" with a single label.\n",
    "The resulting analysis presents the number of attacks per day:\n",
    "\n",
    "| Date_time | Attacks |\n",
    "| --- | --- |\n",
    "|June 4th, 2019 | 82 attacks |\n",
    "|June 5th, 2019 | 124 attacks |\n",
    "|June 6th, 2019 | 117 attacks |\n",
    "|June 7th, 2019 | 121 attacks |\n",
    "|June 8th, 2019 | 118 attacks |\n",
    "| ... (continues with dates up to) |\n",
    "| February 25th, 2020| 649 attacks |\n",
    "| February 26th, 2020| 483 attacks |\n",
    "| February 27th, 2020| 551 attacks |\n",
    "| February 28th, 2020| 580 attacks |\n",
    "| February 29th, 2020| 627 attacks |\n",
    "\n",
    "This temporal series reveals fluctuations in attack intensity over time, with notable spikes and drops in attack occurrences. The observations suggest potential patterns or trends that could be further explored to understand the dynamics of these SSH shell attacks across different periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "sns.lineplot(\n",
    "    data = attacks_per_day,\n",
    "    linestyle='-',\n",
    "    color= 'blue',\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "plt.title('Number of Attacks')\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of attacks\")\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m/%y'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization depicting attack frequencies over time reveals distinct patterns:\n",
    "\n",
    "Between June 2019 and September 2019, there is a conspicuous decrease in attack occurrences, indicating a phase of minimal activity. Subsequently, there is a significant surge in attack instances towards the latter part of 2019, signifying a notable rise in both the frequency and intensity of attacks during this period.\n",
    "\n",
    "This timeline underscores a stark contrast between the relatively quiet phase observed from June to September 2019 and the pronounced escalation in attack activities, particularly notable in the latter months of the year. This shift in trend emphasizes a substantial alteration in attack behavior, marked by an extended period of low activity succeeded by a considerable surge in attack incidents towards the year's end.\n",
    "\n",
    "Moreover, at the beginning of 2020, there is a noticeable decline in attack occurrences once more. This decline follows the heightened activity observed in late 2019, representing a shift from the increased attack rates back to a decreased frequency as the year transitions into its initial months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks_per_year = df_attacks.groupby(df['first_timestamp'].dt.year).size().to_frame(\"Number_attacks\").reset_index()\n",
    "attacks_per_year.rename(columns={\"first_timestamp\": \"Year\"}, inplace=True)\n",
    "attacks_per_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contrast in attack counts between the two years, with 2019 showing a significantly higher number of attacks compared to 2020, can be reasonably attributed to the limited temporal coverage of the dataset for the year 2020. With data available for only two months of 2020, the reduced number of observations in this period is expected and explains the lower count of attacks for that year compared to the extensive records available for 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "sns.barplot(\n",
    "    data = attacks_per_year,\n",
    "    x = \"Year\",\n",
    "    y = \"Number_attacks\",\n",
    "    hue= \"Year\",\n",
    "    palette = ['blue','orange']\n",
    ")\n",
    "\n",
    "plt.ylabel(\"Number of attacks\")\n",
    "plt.title(\"Number of total attacks in 2019 and 2020\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attacks_2019 = df_attacks.loc[df_attacks[\"first_timestamp\"].dt.year == 2019]\n",
    "df_attacks_2019_month = df_attacks_2019.groupby(df_attacks_2019[\"first_timestamp\"].dt.month).size().to_frame(\"Number_attacks_2019_month\").reset_index()\n",
    "\n",
    "df_attacks_2020 = df_attacks.loc[df_attacks[\"first_timestamp\"].dt.year == 2020]\n",
    "df_attacks_2020_month = df_attacks_2020.groupby(df_attacks_2020[\"first_timestamp\"].dt.month).size().to_frame(\"Number_attacks_2020_month\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "fig, axes = plt.subplots(1,2 ,sharey=True)\n",
    "plt.subplot(1,2,1)\n",
    "sns.barplot(\n",
    "    data = df_attacks_2019_month,\n",
    "    x = \"first_timestamp\",\n",
    "    y = \"Number_attacks_2019_month\",\n",
    "    ax=axes[0],\n",
    "    color = 'blue'  \n",
    ")\n",
    "plt.title(\"2019\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Number of attacks\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.barplot(\n",
    "    data = df_attacks_2020_month,\n",
    "    x = \"first_timestamp\",\n",
    "    y = \"Number_attacks_2020_month\",\n",
    "    ax=axes[1],\n",
    "    color = 'orange'\n",
    ")\n",
    "plt.title(\"2020\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Number of attacks\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar charts reveal intriguing trends:\n",
    "\n",
    "- In 2019, there is a noticeable surge in the number of attacks from months 9 to 12. A progressive increase in attack activity is observed during these months, reaching a peak towards the year's end.\n",
    "\n",
    "- At the onset of 2020, in months 1 and 2, a relatively similar frequency of attacks is noted, approximately representing half the number of attacks compared to month 10 in 2019.\n",
    "\n",
    "These patterns outline a significant uptick in attack activity towards the end of 2019, followed by a comparatively steady beginning in 2020 with a considerably lower number of attacks compared to the peak period of the previous year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2. Extract features from the attack sessions. How does the empirical distribution of the number of\n",
    "characters in each session look like? How is the distribution of the number of word per session?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of characters and words in each session\n",
    "number_characters = df['full_session'].apply(lambda x: len([char for char in x if char.isalpha()]))\n",
    "\n",
    "number_words = df['full_session'].apply(lambda x: len(clean_session(x)))\n",
    "\n",
    "data = {\"number_characters\": number_characters, \"number_words\": number_words}\n",
    "df_number_characters_words = pd.DataFrame(data = data)\n",
    "df_number_characters_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "sns.ecdfplot(\n",
    "    data = df_number_characters_words['number_characters'],\n",
    "    log_scale=True\n",
    ")\n",
    "plt.title(\"ECDF for characters\")\n",
    "plt.xlabel(\"Number of characters\")\n",
    "plt.ylabel(\"CDF\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.ecdfplot(\n",
    "    data = df_number_characters_words['number_words'],\n",
    "    log_scale=True\n",
    ")\n",
    "plt.title(\"ECDF for words\")\n",
    "plt.xlabel(\"Number of words\")\n",
    "plt.ylabel(\"CDF\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot, it's possible to see that the distribution of words per session and the distribution of characters per session predominantly concentrate below 2000 words and below 20,000 characters, respectively.\n",
    "\n",
    "To have a clearer idea of the distribution, we decided to limit the x-axis for both plots so that we can closely examine these two distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histograms, it is observed that the distribution of character counts per session centers is concentrated around approximately 350 characters.\n",
    "Moreover, regarding the number of words per session, the distribution frequently peaks around 40 to 48 words. This indicates that sessions often contain this range of word counts, emphasizing a typical occurrence of sessions with this word count range. These insights provide a clear understanding of the common lengths observed within the attack sessions, both in terms of characters and words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3. What are the most common words in the sessions?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining all text from 'full_session' into a single string\n",
    "full_session = ' '.join(df['full_session'])\n",
    "session_cleaned = clean_session(full_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_most_common_words = pd.Series(session_cleaned).value_counts().head(10)\n",
    "most_common_word = df_most_common_words.idxmax()\n",
    "frequency = df_most_common_words.max()\n",
    "word_freq=df_most_common_words.to_dict()\n",
    "most_common_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common word is : 'tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary that holds the frequencies of the top 10 most common words\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This representation emphasizes words based on their frequency in the sessions. Words that appear more frequently will be displayed larger and more prominently within the WordCloud. The interpolation='bilinear' argument enhances the image quality for better clarity. The plt.axis('off') command removes the axis for a cleaner visual appearance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4. How are the intents distributed? How many intents per session do you observe? What are the most common intents? How are the intents distributed in time?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents_df = df[[\"session_id\", \"first_timestamp\", \"Set_Fingerprint\"]]\n",
    "\n",
    "# Expanding the 'Set_Fingerprint' column to individual intents and sessions\n",
    "intents_df_exploted = intents_df.explode('Set_Fingerprint')\n",
    "intents_df_grouped = intents_df_exploted.groupby(\"session_id\").size().to_frame(\"Number_of_intents\")\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.ecdfplot(\n",
    "    data = intents_df_grouped,\n",
    "    legend=False\n",
    ")\n",
    "plt.title(\"ECDF for intents\")\n",
    "plt.xlabel(\"Intents\")\n",
    "plt.ylabel(\"CDF\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = intents_df_exploted.groupby('Set_Fingerprint').size().sort_values(ascending=False).to_frame(\"Number_of_sessions\")\n",
    "sns.set(style=\"darkgrid\")\n",
    "plt.figure(figsize=(6,3))\n",
    "sns.barplot(intents, x=\"Number_of_sessions\", y=intents.index, color=\"blue\", hue_order=intents.index)\n",
    "plt.title('Distribution of Intents')\n",
    "plt.xlabel('Number of sessions')\n",
    "plt.ylabel('Type of Intents')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The presented bar chart shows the most common intentions found in the dataset; Discovery, Persistence and Execution lead the most used type of attacks for each of the sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intents_distribution = intents_df_exploted.groupby([pd.Grouper(key='first_timestamp', freq='D'), 'Set_Fingerprint']).size().to_frame(\"Number_of_intents\").reset_index()\n",
    "df_intents_distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(\n",
    "    data = df_intents_distribution,\n",
    "    x = \"first_timestamp\",\n",
    "    y = \"Number_of_intents\",\n",
    "    hue=\"Set_Fingerprint\",\n",
    "    legend=True\n",
    ")\n",
    "plt.title(\"Distribution of the intents over the time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of the intents\")\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m/%y'))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME WHICH PLOT SHOULD WE USE ?\n",
    "\n",
    "grid = sns.FacetGrid(df_intents_distribution, col=\"Set_Fingerprint\", col_wrap=1, height=4, sharey=False, aspect=2)\n",
    "\n",
    "grid.map(sns.lineplot, \"first_timestamp\", \"Number_of_intents\")\n",
    "\n",
    "grid.set_axis_labels(\"Date\", \"Number of Intents\")\n",
    "grid.set_titles(col_template=\"{col_name}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of intents unfolds as follows:\n",
    "\n",
    "**Defense Evasion**:\n",
    "No significant peaks noticed, with a few attacks observed between July and September 2019, remaining consistently low alongside 'Harmless', 'Impact', and 'Other' intents.\n",
    "\n",
    "**Execution**:\n",
    "Displays a sharp peak towards the end of 2019, notably in the last two months.\n",
    "\n",
    "**Persistence and Discovery**:\n",
    "Showcase an intriguing trend, reaching their highest peaks towards the end of 2019. These intents exhibit the highest frequency, notably surging towards the year-end, reaching maximum levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5. How can text represented numerically? Try to convert the text into numerical representations\n",
    "(vectors) through Bag of Words (BoW)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_session_cleaned = df.copy()\n",
    "df_session_cleaned.update(df_session_cleaned[\"full_session\"].apply(lambda x : clean_session(x)))\n",
    "df_session_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(min_df = 0.05)\n",
    "bow = count_vectorizer.fit_transform(df_session_cleaned[\"full_session\"].apply(lambda x : \" \".join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_session_cleaned_bow = pd.DataFrame(bow.toarray(), index=df_session_cleaned.index, columns = list(count_vectorizer.vocabulary_.keys()))\n",
    "for feature in df_session_cleaned_bow.columns:\n",
    "    df_session_cleaned_bow[feature] = normalize(df_session_cleaned_bow[feature].values.reshape(-1,1), norm=\"l2\", axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_session_cleaned_bow = pd.concat([df_session_cleaned, df_session_cleaned_bow], axis=1)\n",
    "df_session_cleaned_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.6. Associate each word in each attack session with its TF-IDF value (Term Frequency-Inverse Document Frequency)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df = 0.05)\n",
    "tfid = tfidf_vectorizer.fit_transform(df_session_cleaned[\"full_session\"].apply(lambda x : \" \".join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_session_cleaned_tfidf = pd.DataFrame(tfid.toarray(), index=df_session_cleaned.index, columns = list(tfidf_vectorizer.vocabulary_.keys()))\n",
    "df_session_cleaned_tfidf = pd.concat([df_session_cleaned, df_session_cleaned_tfidf], axis=1)\n",
    "df_session_cleaned_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Correlation Matrix</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_bow = df_session_cleaned_bow.drop(columns=[\"session_id\", \"full_session\", \"first_timestamp\", \"Set_Fingerprint\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix_bow = df_features_bow.corr().abs()\n",
    "plt.figure(figsize=(50,50))\n",
    "sns.heatmap(correlation_matrix_bow, cmap='Blues', annot=True, vmin=.0, vmax=1, cbar_kws={'label':'Correlation'})\n",
    "plt.title('Correlation matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_tfidf = df_session_cleaned_tfidf.drop(columns=[\"session_id\", \"full_session\", \"first_timestamp\", \"Set_Fingerprint\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix_tfidf = df_features_tfidf.corr().abs()\n",
    "\n",
    "plt.figure(figsize=(50,50))\n",
    "sns.heatmap(correlation_matrix_tfidf, cmap='Blues', annot=True, vmin=.0, vmax=1, cbar_kws={'label':'Correlation'})\n",
    "plt.title('Correlation matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_bow = PCA()\n",
    "pca_bow.fit(df_features_bow)\n",
    "explained_variance_bow = pca_bow.explained_variance_ratio_\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.lineplot(\n",
    "    data = explained_variance_bow,\n",
    "    marker=\"o\"\n",
    ")\n",
    "plt.xticks(range(0, len(explained_variance_bow), 5))\n",
    "plt.xlabel(\"Principal Components\")\n",
    "plt.ylabel(\"Explained variance\")\n",
    "plt.title(\"PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_bow = PCA(n_components=4)\n",
    "pca_result_bow = pca_bow.fit_transform(df_features_bow)\n",
    "pca_result_bow = pd.DataFrame(pca_result_bow, columns=[f'PC{i}' for i in range(pca_bow.n_components_)])\n",
    "pca_result_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_tfidf = PCA()\n",
    "pca_tfidf.fit(df_features_tfidf)\n",
    "explained_variance_tfidf = pca_tfidf.explained_variance_ratio_\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(\n",
    "    data = explained_variance_tfidf,\n",
    "    marker=\"o\"\n",
    ")\n",
    "plt.xticks(range(0, len(explained_variance_tfidf), 5))\n",
    "plt.xlabel(\"Principal Components\")\n",
    "plt.ylabel(\"Explained variance\")\n",
    "plt.title(\"PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_tfidf = PCA(n_components=3)\n",
    "pca_result_tfidf = pca_tfidf.fit_transform(df_features_tfidf)\n",
    "pca_result_tfidf = pd.DataFrame(pca_result_tfidf, columns=[f'PC{i}' for i in range(pca_tfidf.n_components_)])\n",
    "pca_result_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<left><b><font size=4>Section 2 â€“ Supervised Learning â€“ Classification<b><left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix, make_scorer, f1_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"text-align: justify\"> Classify the tactics of an attack session, based on the used words in the text and also possibly on time. Notice that each session have multiple labels. Hence you can decompose the problem into multiple binary classification problems. For each attack session, you have to solve the 7 binary classification problem, one for each possible label {'Persistence', 'Discovery', 'Defense Evasion', 'Execution', 'Impact', 'Other', 'Harmless'}. </div><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Perform a split to segment the dataset into training and test dataset. If you want to standardize your dataset, fit the scaler on training set and transforming both training and test. Notice that the sklearn implementation of tf-idf already performs the standardization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_feature = pca_result_tfidf\n",
    "y_feature =  df_session_cleaned[\"Set_Fingerprint\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_feature,\n",
    "    y_feature,\n",
    "    train_size = 0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The size of trainning set is:', len(X_train))\n",
    "print('The size of test set is:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standardization of the Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the TF-IDF pre-processing was applied previously to all the sessions, the data considered as features was already standardized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standardization of the Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "y_train_mlb = mlb.fit_transform(y_train)\n",
    "y_test_mlb = mlb.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing Techniques** \n",
    "<br>\n",
    "<div style=\"text-align: justify\"> A <b>MultiLabelBinarizer</b> is a transformer that is used for multi-label classification problems, in order to handle the cases where each sample belongs to multiple classes simultaneously. The purpose of MultiLabelBinarizer is to convert a collection of sequences of labels into a binary matrix format. The binary classification of each label in the 'Set_Fingerprint' column was performed by converting the multi-class label matrix into a binary matrix, where each column represents one of the possible classes and each row represents one instance. </div><br>\n",
    "\n",
    "<div style=\"text-align: justify\"> <b>TF-IDF </b> (explain technique here) </div><br>\n",
    "\n",
    "<div style=\"text-align: justify\"> MultiLabelBinarizer is used to handle categorical variables before fitting a model, as most machine learning algorithms can only handle numerical data.</div><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Choose at least 2 ML methods, and perform the model training, with default parameter\n",
    "configuration, evaluating the performance on both training and test set. Output the confusion\n",
    "matrix and classification report. Do you observe overfitting or under-fitting? Which model\n",
    "generates the best performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><left><b><font size=4> Random Forest (RF)<b><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">Random Forest (RF) serves as a classification model that constructs a collection of decision trees (DT) using a randomly chosen subset of the given training set. The model aggregates the individual decisions made by each decision tree and combines their votes to make the ultimate prediction.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestClassifier(n_estimators=100) \n",
    "\n",
    "st = time.time()\n",
    "model_rf.fit(X_train, y_train_mlb)\n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "print(f\"Time to train the model: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predictions = model_rf.predict(X_train)\n",
    "y_test_predictions = model_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the train data\n",
    "accu = model_rf.score(X_train, y_train_mlb)\n",
    "accuracy = metrics.accuracy_score(y_train_mlb, y_train_predictions)\n",
    "print(f\"Accuracy of the 'Random Forest' model for the training set: {accuracy:.2f}, {accu:.2f}\")\n",
    "\n",
    "# Evaluate the model's performance on the test data\n",
    "accu = model_rf.score(X_test, y_test_mlb)\n",
    "accuracy = metrics.accuracy_score(y_test_mlb, y_test_predictions)\n",
    "print(f\"Accuracy of the 'Random Forest' model for test set: {accuracy:.2f}, {accu:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<left><b><font size=3 >Classification Report<b><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME WE SHOULD SPLIT BETTER THE TRAIN AND THE TEST SET, THE WARNING COMES OUT BECAUSE y_train_mlb AND y_train_predictions DON'T HAVE THE SAME LABELS (IMPACT AND OTHER ARE VERY FEW)\n",
    "# Evaluate performance on training set\n",
    "report_training = classification_report(y_train_mlb, y_train_predictions, target_names=mlb.classes_, output_dict=True)\n",
    "df_report_training = pd.DataFrame(report_training).transpose()\n",
    "df_report_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation Set (Test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance on test set\n",
    "report_test = classification_report(y_test_mlb, y_test_predictions, target_names=mlb.classes_, output_dict=True)\n",
    "df_report_test = pd.DataFrame(report_test).transpose()\n",
    "df_report_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<left><b><font size=3> Confusion Matrix <b><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_rf_train = multilabel_confusion_matrix(y_train_mlb, y_train_predictions)\n",
    "\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    print(f\"Confusion Matrix for '{label}':\")\n",
    "    print(confusion_rf_train[i], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.title(f\"Confusion Matrix for '{label}'\")\n",
    "    plt.imshow(confusion_rf_train[i], cmap='Blues', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.xticks(np.arange(2), ['Negative', 'Positive'])\n",
    "    plt.yticks(np.arange(2), ['Negative', 'Positive'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_rf_test = multilabel_confusion_matrix(y_test_mlb, y_test_predictions)\n",
    "\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    print(f\"Confusion Matrix for '{label}':\")\n",
    "    print(confusion_rf_test[i], \"\\n\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.title(f\"Confusion Matrix for '{label}' in Test Set\")\n",
    "    plt.imshow(confusion_rf_test[i], cmap='Oranges', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.xticks(np.arange(2), ['Negative', 'Positive'])\n",
    "    plt.yticks(np.arange(2), ['Negative', 'Positive'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<left><b><font size=4>K-Nearest Neighbors (KNN)<b><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">K-Nearest Neighbors (KNN) operates as a supervised learning classifier that relies on the concept of proximity to perform classifications or predictions for individual data points. Its fundamental principle is grounded in the notion that similar data points tend to cluster together. In the context of classification tasks, KNN assigns a class label to a data point by considering the majority vote of its nearest neighbors. Put simply, it selects the label that is most prevalent among the neighboring data points in close proximity to the one being evaluated.</div><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "st = time.time()\n",
    "knn.fit(X_train, y_train_mlb)\n",
    "et = time.time()\n",
    "\n",
    "elapsed_time = et - st\n",
    "print(f\"Time to train the model: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = knn.predict(X_train) \n",
    "predictions_test = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = knn.score(X_train, y_train_mlb)\n",
    "print(f\"Accuracy of the k-NN model for the training set: {accuracy:.2f}\")\n",
    "\n",
    "accuracy = knn.score(X_test, y_test_mlb)\n",
    "print(f\"Accuracy of the k-NN model for the test set: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<left><b><font size=3 >Classification Report<b><left>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report for training set\n",
    "report_train_knn = classification_report(y_train_mlb, predictions_train, target_names=mlb.classes_, output_dict=True)\n",
    "df_report_train = pd.DataFrame(report_train_knn).transpose()\n",
    "print(\"Classification Report for Trainning set:\")\n",
    "df_report_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report for test set\n",
    "report_test_knn = classification_report(y_test_mlb, predictions_test, target_names=mlb.classes_, output_dict=True)\n",
    "df_report_test_knn = pd.DataFrame(report_test_knn).transpose()\n",
    "print(\"Classification Report for Test set:\")\n",
    "df_report_test_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<left><b><font size=3> Confusion Matrix <b><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code presented below prints a series of confusion matrices for each class, displaying True-Positive (top-left), False-Negative (bottom-left), False-Positive (top-right), and True-Negative (bottom-right) counts.\n",
    "- True Positives (TP): Predicted correctly as positive.\n",
    "- False Positives (FP): Predicted as positive but actually negative.\n",
    "- False Negatives (FN): Predicted as negative but actually positive.\n",
    "- True Negatives (TN): Predicted correctly as negative.\n",
    "\n",
    "Each value in the confusion matrix represents the count of instances falling into these categories for a specific label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with a confusion matrix and classification report\n",
    "confusion_knn_test = multilabel_confusion_matrix(y_test_mlb, predictions_test)\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    print(f\"Confusion Matrix for {label}:\")\n",
    "    print(confusion_knn_test[i], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "for i, label in enumerate(mlb.classes_):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.title(f\"Confusion Matrix for '{label}'\")\n",
    "    plt.imshow(confusion_knn_test[i], cmap='Oranges', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.xticks(np.arange(2), ['Negative', 'Positive'])\n",
    "    plt.yticks(np.arange(2), ['Negative', 'Positive'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> To know whether the model is underfitting or overfitting, we must first define these two terms. <br>\n",
    "<div style=\"text-align: justify\"><br><b>Underfitting</b> occurs when a model is too simple to capture the underlying patterns in the training data, resulting in poor performance even on the training set.  One indicator used to identify this modelling error is to look at the results; if both training and validation/testing performance are poor, the model is considered to be underfitted.</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\"><b>Overfitting</b> occurs when a model not only learns the underlying patterns in the training data, but also captures noise and random fluctuations, causing it to perform poorly on new and unknown data. One indicator used to identify this modelling error is to look at the results, if the model performs well on training data but poorly on validation or test data, it is considered overfitted.</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">Once these two terms have been defined, it is possible to move on to the results obtained for both models. It is important to mention that, for both tests performed with the different classifiers, the default parameters were used.</div><br>\n",
    "<div style=\"text-align: justify\">For the <b>Random Forest (RF)</b> case, the default parameter implied the number of estimators equal to 100, and the tree depth was set to <i>'None'</i>. On the other hand, for the <b>K-Nearest Neighbor (KNN)</b> classifier, the number of estimators was set to 5, the leaf size to 30 and the type of distance calculation was <i>'Euclidean distance'</i> (p=2), all default parameters of the classifier.</div><br>\n",
    "    \n",
    "<b> Random Forest (RF)</b>\n",
    "<div style=\"text-align: justify\"> In the classification report of the training and test sets, for most of the classes, accuracy, recall and F1 score are slightly lower in the test set compared to the training set. This was expected, as models tend to generalise slightly worse with unseen data. However, the drop in performance is not significant, which indicates that the model still performs well on the test set. Furthermore, the accuracy obtained for the training set was 99%, while 98% for the test set.<br><br>\n",
    "    \n",
    "<div style=\"text-align: justify\">As can be seen in the validation set report, the model was not able to correctly classify instances of the <i>'Impact'</i> class, performing very poorly (0%) on all precision, recall and F1 score metrics. This result could be due to the default parameters set to train the model.  Tree depth is one of the most important parameter for tuning the model, as it sets the stop condition that limits the number of splits or levels deep a decision tree can go.</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">To enhance the classifier results, it is necessary to adjust the maximum depth of the decision trees when performing hyperparameter fitting for a random forest model. The <i>'weighted avg'</i> metric also showed a decrease in performance on the test set, indicating that the model does not perform as well on the test set across all classes, considering the distribution of classes. Overall, the performance metrics on the test set remain high, indicating that the overfitting is not critical. </div><br>\n",
    "\n",
    "<b> K-Nearest Neighbor (KNN)</b>\n",
    "\n",
    "<div style=\"text-align: justify\">On the other hand, the classification report obtained for the KNN classifier showed a small difference between the training set and the validation set, for the test set the values obtained for each of the metrics; accuracy, recall, f1-score, were slightly lower compared to the results obtained for the training set.</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\"> For the <i>'Impact'</i> class, the metrics derived in the training set were significantly lower compared to the validation set, for all metrics. This improvement in the accuracy, recall and f1-score parameters for the <i>'Impact'</i> class in the test set indicates that the model's predictions for this class are more accurate and reliable when evaluated with new, unseen data.  Producing a recall of 50%, which means, that the model only correctly predicted this class for 50% of the evaluated intents. </div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">The difference obtained in this class for the training and validation sets suggests that the model did not sufficiently fit the <i>'Impact'</i> class during training, and then after the selection of the nearest neighbour from the test set, the model adjusted its predictions to better capture the features of the <i>'Impact'</i> class. However, it is important to note that the KNN classifier does not fit the data, it does not learn from the model, it only calculates the distance to the nearest points and selects the class according to the majority result of the nearest neighbours.</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\"> In general, the observed results do not indicate underfitting or overfitting, in fact, the average accuracy obtained in the classification report was 99%, matching with the obtained in the calculated accuracy score (98% for both sets). The high values of the <i>'micro-average'</i> in both sets suggest a good overall performance of the model. While the <i>'macro-average'</i> values are higher only in the test set, indicating that the model performs better after calculating the Euclidean distance of each point.</div><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Tune the hyper-parameters of the models through cross-validation. How do performance vary?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment, a value of 5 K-fold cross-validation was selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = make_scorer(f1_score, average='macro')\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(X_train)\n",
    "\n",
    "for i, (train_index,test_index) in enumerate(kf.split(X_train)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={test_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(model_rf, X_train, y_train_mlb, cv=kf, scoring = scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(knn, X_train, y_train_mlb, cv=kf, scoring=scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">Hyperparameters are settings that control the learning process of machine learning models. While the parameters are learned during the training process, the hyperparameters are set before the training starts. Therefore, in order to find the parameters that best fit the performance of the model, the GridSearch technique was applied. This technique applies all possible combinations of hyperparameters, resulting in a set of parameters that will improve the performance of the model.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "params = {'n_estimators': [10, 50, 80], 'max_depth': [15, 25, 50] } #'criterion' :['gini', 'entropy']}\n",
    "\n",
    "# Instantiate the grid search model\n",
    "gs_rf = GridSearchCV(model_rf, param_grid = params, scoring='f1_macro', cv = 5, verbose = 1) \n",
    "# scoring='accuracy'\n",
    "# cv: that's the number of fold for the cross-validation\n",
    "# verbose: specifies the verbosity level of the GridSearchCV object. \n",
    "\n",
    "# Trainning the model\n",
    "st = time.time()\n",
    "gs_rf.fit(X_train, y_train_mlb)\n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "print(f\"Time to train the model: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_rf = gs_rf.best_params_\n",
    "print(f\"The best combination of parameters the Grid Search has found is: {best_params_rf}\")\n",
    "print(\"Best F1-Score: {:.2f}\".format(gs_rf.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ask to the prof which graph we should use -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the f1 macro reached for each combination\n",
    "y = gs_rf.cv_results_[\"mean_test_score\"].tolist()\n",
    "x = [i for i in range (1, len(y)+1)]\n",
    "mean_test_score_df = pd.DataFrame()\n",
    "mean_test_score_df[\"f1_macro\"] = y\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x = mean_test_score_df.index, y = \"f1_macro\", data = mean_test_score_df, color='blue')\n",
    "\n",
    "# Add a title and labels to the plot\n",
    "plt.title('F1-macro Scores for Different Parameters')\n",
    "plt.xlabel('Combination')\n",
    "plt.ylabel('F1-macro Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rf = pd.DataFrame(gs_rf.cv_results_)\n",
    "results_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a pivot table \n",
    "scores_rf = results_rf.pivot(index='param_max_depth', columns='param_n_estimators', values='mean_test_score')\n",
    "scores_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(scores_rf, annot=True, cmap='viridis', fmt='.5g')\n",
    "plt.xlabel('param_max_depth')\n",
    "plt.ylabel('param_n_estimators')\n",
    "plt.title('Mean F1-Score over all folds for each combination of parameters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_neighbors': [5, 7, 9], 'leaf_size': [5, 15, 20]} #'metric': ['euclidean', 'manhattan']} \n",
    "grid_search_knn = GridSearchCV(knn, params, scoring='f1_macro', cv = 5, verbose=1)\n",
    "# scoring = 'accuracy'\n",
    "\n",
    "st = time.time()\n",
    "grid_search_knn.fit(X_train, y_train_mlb)\n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "print(f\"Time to train the model: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_knn = grid_search_knn.best_params_\n",
    "print(f\"The best combination of parameters the Grid Search has found is: {best_params_knn}\")\n",
    "print(\"Best F1-Score: {:.2f}\".format(grid_search_knn.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aks to the prof which graph we should use --------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the f1 macro reached for each combination\n",
    "y = grid_search_knn.cv_results_[\"mean_test_score\"].tolist()\n",
    "x = [i for i in range (1, len(y)+1)]\n",
    "mean_test_score_df = pd.DataFrame()\n",
    "mean_test_score_df[\"f1_macro\"] = y\n",
    "#print(mean_test_score_df)\n",
    "\n",
    "sns.barplot(x = mean_test_score_df.index, y = \"f1_macro\", data = mean_test_score_df, color='blue')\n",
    "\n",
    "# Add a title and labels to the plot\n",
    "plt.title('F1-macro Scores for Different Parameters')\n",
    "plt.xlabel('Combination')\n",
    "plt.ylabel('F1-macro Score')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_knn = pd.DataFrame(grid_search_knn.cv_results_)\n",
    "results_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a pivot table before create the heatmap\n",
    "scores_knn = results_knn.pivot(index='param_leaf_size', columns='param_n_neighbors', values='mean_test_score')\n",
    "scores_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(scores_knn, annot=True, cmap='viridis', fmt='.5g')\n",
    "plt.xlabel('param_n_neighbors')\n",
    "plt.ylabel('param_leaf_size')\n",
    "plt.title('Mean F1-score over all folds for each combination of parameters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4. Comments on the results for each on the intents.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">The aim of this analysis is to assess the predictive capability of two models in classifying attack labels. The models will be evaluated based on the hyperparameters identified previously.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation of Random Forest with tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with the hyperparameters\n",
    "model_rf_tunned = RandomForestClassifier(n_estimators = 25, max_depth = 10) # (n_estimators=30, max_depth=50)\n",
    "\n",
    "st = time.time()\n",
    "# Trainning the model\n",
    "model_rf_tunned.fit(X_train, y_train_mlb)\n",
    "et = time.time()\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "print(f'Time to train the model:', elapsed_time,'seconds','\\n')\n",
    "\n",
    "# Predictions on test set\n",
    "y_test_pred_tune = model_rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance on the test data\n",
    "accuracy = metrics.accuracy_score(y_test_mlb, y_test_pred_tune)\n",
    "print(f\"Accuracy of the 'Random Forest' model for test set: {accuracy:.2f}\",'\\n')\n",
    "\n",
    "# Evaluate performance on test set\n",
    "report_test_tune = classification_report(y_test_mlb, y_test_pred_tune, target_names=mlb.classes_, output_dict=True)\n",
    "df_report_test_tune = pd.DataFrame(report_test_tune).transpose()\n",
    "print(f'         Classification Report Trainning Set', '\\n')\n",
    "print(df_report_test_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the heatmap of the correlation matrix\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.heatmap(df_report_test_tune.loc[\"Defense Evasion\" : \"Persistence\"], cmap='Blues', annot=True, vmin=.0, vmax=1,fmt='.3f')\n",
    "plt.xlabel('Intents')\n",
    "plt.ylabel('Evaluation technique')\n",
    "plt.title('Intents classification report')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evalutaing of K-Nearest Neighbors with tuned parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the k-NN model\n",
    "knn_tune = KNeighborsClassifier(leaf_size=5, n_neighbors=5)\n",
    "\n",
    "# Train the model on the training data\n",
    "t = time.time()\n",
    "knn_tune.fit(X_train, y_train_mlb)\n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "print(f\"Time to train the model: {elapsed_time} seconds\")\n",
    "\n",
    "# Generate predictions on the test set\n",
    "predictions_knn_tune = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance on the test data\n",
    "accuracy_knn_tune = knn.score(X_test, y_test_mlb)\n",
    "print(f\"Accuracy of the k-NN model: {accuracy_knn_tune:.2f}\",'\\n')\n",
    "\n",
    "report_knn_tune = classification_report(y_test_mlb, predictions_knn_tune, target_names = mlb.classes_, output_dict=True)\n",
    "df_report_knn_tune = pd.DataFrame(report_knn_tune).transpose()\n",
    "print(\"              Classification Report for KNN\",'\\n')\n",
    "print(df_report_knn_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the heatmap of the correlation matrix\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.heatmap(df_report_knn_tune.loc[\"Defense Evasion\" : \"Persistence\"], cmap='Blues', annot=True, vmin=.0, vmax=1,fmt='.3f')\n",
    "plt.xlabel('Intents')\n",
    "plt.ylabel('Evaluation technique')\n",
    "plt.title('Classification report')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to write how the hyperparameter tunning improves the result; especially in the intent Impact, for both models... I'll do it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5 Explore the possible features: try combining features differently, e.g., does tf-idf improve or worsen performance? Think about the problem and summarize the ways you have tried (even those that did not work).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"><b>First Attempt</b></div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">For the first attempt, 33 features were used for the training set. The classifiers selected to perform the predictions were 'Random Forest' and 'K-Nearest Neighbor' with both models using the default parameters. In the performance evaluation, an accuracy of 98% was obtained for both models in the validation set.  While in the training set it reached 99% for RF and 98% for KNN. The decrease in accuracy achieved by RF in the validation set suggests a slight overfitting, however, it is a tolerable value that assumes that the model still performs well on the test set.</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">According to the classification report, for almost all attempts the values of precision, recall and f1-scores reached high percentages, around 98.9%.  Except for the 'Impact' and 'Harmless' intents when applying RF as a model, the results obtained for these classes were quite lower compared to those obtained for the other classes, both in the training set and in the validation set, where 0% was obtained in each metric. The model presented a very poor performance when trying to classify these two classes.</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">As for tuning techniques, K-fold cross-correlation and Grid Search were applied to see if the results obtained previously could be improved. However, when looking at the results obtained with 5 folds, the performance decreased considerably reaching an average of 77% accuracy. Suggests that the model may be in overfitting. </div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">Therefore, for the next attempt we consider a reduction of the dimensionality of the data set to improve generalization.</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\"><b>Second attempt</b></div><br>\n",
    "<div style=\"text-align: justify\">For the second attempt, the number of features was reduced to only 12 for the training set.  The classifiers used in the previous attempt, RF and KNN, were kept for this evaluation with both using their default parameters. The results obtained in the performance evaluation, the accuracy did not change from the previous attempt, reaching 98% for the RF case and 98% for the KNN for both the training and validation sets, indicating that the models continue to make good predictions for the classes.<br>\n",
    "<br>\n",
    "<div style=\"text-align: justify\">If we take a look at the classification report for almost all attempts, the values for accuracy, recall and f1-scores reached high percentages, around 98%. Nevertheless, the performance of the 'Impact' class was very poor in both models for the classification obtained in the validation data, with 0% for every metric (precision, recall and f1-score), indicating that the number of features selected to train both models was not sufficient to be able to correctly classify this class.<br><br>\n",
    "<div style=\"text-align: justify\">In both training and validation reports, the F1 score obtained for the classes \"Impact\" and \"Harmless\" was very low compared to the other classes. It seems that for both models, these two classes are the most difficult to classify correctly. This could be due to the fact that, the number of data selected during the splitting of the training set did not cover enough samples of these two classes, as they are the least sampled attempts of the whole dataset. </div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">After conducting multiple tests and adjusting the number of features in the training set, the results showed similar levels of accuracy. However, upon further analysis of the classification report parameters, it was found that the highest metrics for accuracy, recall, and F1-score were achieved with a number of features greater than 22. The F1-score parameter indicated that the models performed better in classifying samples belonging to the 'Impact' and 'Harmless' classes, which were more difficult to detect in almost all the tests. </div><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <left><b><font size=4>Section 3 â€“ Unsupervised Learning â€“ Clustering<b><left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">Cluster the attacks according to their characteristics. Choose at least 2 Clustering Algorithms, and for each of them solve the following points.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded = result_df_corr.explode(\"Set_Fingerprint\").reset_index()\n",
    "df_exploded = df_exploded.drop(columns=['index'])\n",
    "#features_exploted = result_df_exploded.drop(columns=[\"session_id\", \"full_session\", \"first_timestamp\", \"Set_Fingerprint\", \"bag_of_words\", \"tfid\"])\n",
    "\n",
    "# We should use the stardardized data (X and y features)\n",
    "labels = df_exploded[\"Set_Fingerprint\"]\n",
    "result_df_exploded = df_exploded.filter(features_names)\n",
    "result_df_exploded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> Two clustering algorithms will be implemented to our dataset, the chosen were MiniBatchK-Means and Gaussian Mixture Model (GMM). <br>\n",
    "First of all we have to select the features to be evaluate, <b>'Pricipal Component Analysis (PCA)'</b> was used to reduce the dimensionality of the data.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction by applying 'PCA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality using PCA----------------------------------------\n",
    "pca = PCA(n_components=10)        # Adjust the number of components as needed\n",
    "reduced_data = pca.fit_transform(result_df_exploded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-Means with n_clusters = 3\n",
    "# cl_labels3 = kmeans.fit_predict(features_exploted) # Get clusters ID\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "cl_labels3 = kmeans.fit(reduced_data) # Using the features selected\n",
    "\n",
    "# print the clustered labels\n",
    "print('The clustered labels are:\\n', kmeans.labels_)\n",
    "print()\n",
    "\n",
    "# print the centroid of each feature for each cluster\n",
    "print('The centroids are:\\n', kmeans.cluster_centers_)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised metric  (it takes approx 2500 sec o 3000 sec)\n",
    "silhouette  = silhouette_score(reduced_data, kmeans.labels_)\n",
    "\n",
    "# Supervised metrics\n",
    "ri = rand_score(np.ravel(labels), kmeans.labels_)\n",
    "ari = adjusted_rand_score(np.ravel(labels), kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('k-Means with 3 clusters')\n",
    "(unique, counts)=np.unique(kmeans.labels_, return_counts=True)\n",
    "print(\"Size of each cluster: \", counts)\n",
    "print(f'k_means clustering error: {round(kmeans.inertia_, 2)}')\n",
    "print(f'Silhouette: {round(silhouette, 2)}')\n",
    "print(f'RI: {round(ri, 2)}')\n",
    "print(f'ARI: {round(ari, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian mixture model (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=3)\n",
    "gmm.fit(result_df_exploded)           # Get clusters ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the clustered labels\n",
    "gmm_labels = gmm.predict(result_df_exploded)\n",
    "print('The clustered labels are:\\n', gmm_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised metric\n",
    "silhouette  = silhouette_score(result_df_exploded, gmm_labels)\n",
    "log_l = gmm.score(result_df_exploded)\n",
    "\n",
    "# Supervised metrics\n",
    "ri = rand_score(np.ravel(labels), gmm_labels)\n",
    "ari = adjusted_rand_score(np.ravel(labels), gmm_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report effective size\n",
    "print(\"Effetive size of each cluster: \", gmm.weights_)\n",
    "# report usupervised and supervised metric\n",
    "print(f'GMM total log-likelihood score: {round(log_l, 2)}')\n",
    "print(f'Silhouette: {round(silhouette, 2)}')\n",
    "print(f'RI: {round(ri, 2)}')\n",
    "print(f'ARI: {round(ari, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1. Determine the number of clusters: This can be done using methods like the elbow method or\n",
    "silhouette analysis. Explain your reasoning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> The clustering error is strongly dependent form the number of cluster. \n",
    "The best situation, with the lowest error possible, is obiuously when we have the same number of clusters of the datapoins. Actually in this situation we're not even performing clustering, but the clustering error will be zero.<br>\n",
    "<br>\n",
    "Now we want to find the number of clusters that gives us the best results, so with the lowest clustering error. \n",
    "We'll follow 2 approches for each clustering algorithm applied, elbow method and validation error. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Elbow Method  (260 sec)\n",
    "n_cluster_list=[]\n",
    "inertia_list=[]\n",
    "shs_list = []\n",
    "ri_list = []\n",
    "ari_list = []\n",
    "\n",
    "for n_clusters in range(3, 16):\n",
    "    kmeans_new = KMeans(n_clusters=n_clusters)\n",
    "    cl_labels = kmeans_new.fit_predict(reduced_data)\n",
    "    \n",
    "    # Unsupervised metric  (If we evaluate the silhoutte it takes too long)\n",
    "    # silhouette = silhouette_score(reduced_data, cl_labels)\n",
    "    # n_cluster_list.append(n_clusters)\n",
    "    # shs_list.append(silhouette)\n",
    "    \n",
    "   # Rand Index and Adjusted Rand Index:\n",
    "    ri_list.append(rand_score(np.ravel(labels), cl_labels))\n",
    "    ari_list.append(adjusted_rand_score(np.ravel(labels), cl_labels))\n",
    "    \n",
    "    # For Elbow Method (Inertia):\n",
    "    inertia_list.append(kmeans_new.inertia_)\n",
    "    n_cluster_list.append(n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elbow Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "# Plot k-Means clustering error \n",
    "# Set up Seaborn style\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(n_cluster_list, inertia_list, marker='o', markersize=5, color='blue')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('k-Means clustering error')\n",
    "plt.title('k-Means Clustering Error Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RI\n",
    "# Set up Seaborn style\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(n_cluster_list, ri_list, marker='o', markersize=5, color='blue')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('RI')\n",
    "plt.title('Rand Index Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ARI\n",
    "# Set up Seaborn style\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(n_cluster_list, ari_list, marker='o', markersize=5, color='blue')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('ARI')\n",
    "plt.title('Adjusted Rand Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THE CODE BELOW: IT's used if we would have chosen to perform the silhouette analysis, but it is not mandatory, of course we have to justify why we do not choose this one, computational time and so on...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette\n",
    "# Get n_clusters leading to the highest silhouette\n",
    "best_sh = np.max(shs_list)\n",
    "best_n = n_cluster_list[np.argmax(shs_list)]\n",
    "print(\"best k: \",best_n, \" with corresponding silhouette: \", best_sh)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(n_cluster_list,shs_list, marker='o', markersize=5)\n",
    "plt.scatter(best_n, best_sh, color='r', marker='x', s=90)\n",
    "plt.grid()\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Mixture Model (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cluster_list=[]\n",
    "shs_list = []\n",
    "ri_list = []\n",
    "ari_list = []\n",
    "log_l_list=[]\n",
    "\n",
    "for n_clusters in range(3, 16):\n",
    "    gmm = GaussianMixture(n_components=n_clusters)\n",
    "    cl_labels = gmm.fit_predict(X_s)\n",
    "    \n",
    "    # silhouette  = silhouette_score(X_s, cl_labels)\n",
    "    # n_cluster_list.append(n_clusters)\n",
    "    # shs_list.append(silhouette)\n",
    "    \n",
    "    ri_list.append(rand_score(np.ravel(y), cl_labels))\n",
    "    ari_list.append(adjusted_rand_score(np.ravel(y), cl_labels))\n",
    "    log_l_list.append(gmm.score(X_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elbow Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Has the performance improved also on the other metrics? Plot the other metrics for the different values of n_cluster.\n",
    "# Set up Seaborn style\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# Plot GMM total log-likelihood score\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(n_cluster_list,log_l_list, marker='o', markersize=5)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('GMM total log-likelihood score')\n",
    "plt.show()\n",
    "\n",
    "# Plot ARI\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(n_cluster_list,ari_list, marker='o', markersize=5)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('ARI')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Plot the silhouette score for the different values of n_cluster. \n",
    "# What is the best value of k, leading to the highest Silhouette.\n",
    "\n",
    "# Get n_clusters leading to the highest silhouette\n",
    "best_sh= np.max(shs_list)\n",
    "best_n=n_cluster_list[np.argmax(shs_list)]\n",
    "print(\"best k: \",best_n, \" with corresponding silhouette: \", best_sh)\n",
    "\n",
    "# Plot\n",
    "# Set up Seaborn style\n",
    "sns.set(style=\"darkgrid\")\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(n_cluster_list,shs_list, marker='o', markersize=5)\n",
    "plt.scatter(best_n, best_sh, color='r', marker='x', s=90)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2. Tune other hyper-parameters, if any.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tunning the hyperparameter of K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Elbow Method\n",
    "n_cluster_list=[]\n",
    "inertia_list=[]\n",
    "shs_list = []\n",
    "ri_list = []\n",
    "ari_list = []\n",
    "\n",
    "for n_clusters in range(3, 16):\n",
    "    kmeans_tunning = KMeans(n_clusters=n_clusters, init='k-means++', random_state=None, n_init=1)\n",
    "    labels_tunning = kmeans_tunning.fit_predict(reduced_data) # Using the features selected\n",
    "    # kmeans_new = KMeans(n_clusters=n_clusters)\n",
    "    # cl_labels = kmeans_new.fit_predict(reduced_data)\n",
    "    \n",
    "    # Rand Index and Adjusted Rand Index:\n",
    "    ri_list.append(rand_score(np.ravel(labels), labels_tunning))\n",
    "    ari_list.append(adjusted_rand_score(np.ravel(labels), labels_tunning))\n",
    "    \n",
    "    # For Elbow Method (Inertia):\n",
    "    inertia_list.append(kmeans_tunning.inertia_)\n",
    "    n_cluster_list.append(n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "# Plot k-Means clustering error \n",
    "# Set up Seaborn style\n",
    "sns.set(style=\"darkgrid\")\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(n_cluster_list, inertia_list, marker='o', markersize=5, color='orange')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('k-Means clustering error')\n",
    "#plt.title('k-Means Clustering Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RI\n",
    "# Set up Seaborn style\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(n_cluster_list, ri_list[:13], marker='o', markersize=5, color='orange')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('RI')\n",
    "plt.title('Rand Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ARI\n",
    "# Set up Seaborn style\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(n_cluster_list, ari_list, marker='o', markersize=5, color='orange')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('ARI')\n",
    "plt.title('Adjusted Rand Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3. Visualize the clusters through t-SNE visualization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4. Cluster analysis. Analyze the characteristics of each cluster. This might involve examining the most frequent words in each cluster (try word cloud). Try to understand which are the most\n",
    "representative.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5. Do clusters reflect intent division, i.e., are the clusters homogeneous in terms of intents? How are intents divided into the clusters?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.6. Find clusters of similar attacks, study their sessions and try to associate with them specific categories of attacks (more fine grained than the ones of MITRE ATT&CK Tactics). As an example, see the image below, where we perform a similar exercise (through graph community detections). NOTE: you do not have to do this exercise for all the clusters, but only on some examples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><left><b><font size=4>Section 4 â€“ Language Models exploration<b><left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from keras.models import Sequential, clone_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><left><b><font size=4>Section 4 â€“ Language Models exploration<b><left>\n",
    "<div style=\"text-align: justify\">Experiment language models for solving the same supervised task as in Section 2. In this task, the objective is to harness the capabilities of language models like Bert or Word2Vec, for supervised learning (assign intents to sessions). \n",
    "<br><br> Two interesting concepts play a role when we use neural networks:\n",
    "<div style=\"text-align: justify\"><br><b>1)</b> It is possible to do transfer learning, i.e., to take a model that have been trained with other enormous datasets by Big Tech companies, and we can do fine-tuning i.e., to train this model starting from its pre-trained version.\n",
    "<br><b>2)</b> In NLP tasks, words/documents are transformed into vectors (encoding) and this task is Unsupervised, so we can use a much larger amount of data.\n",
    "</div>\n",
    "<div style=\"text-align: justify\"><b>4.1. If you choose Doc2Vec: pretrain Doc2Vec on body column of the session text. If you chose Bert: take the pretrained Bert model like in this example. (NB: In this tutorial they used BertForSequenceClassification, but if you want to continue with step 2, you must take an other Bert implementation from HuggingFace)</div></b>\n",
    "\n",
    "#### Data Preparation and Cleaning\n",
    "##### Step 1\n",
    "The initial phase involved loading the dataset from the 'ssh_attacks.parquet' file. To ensure data quality, a cleaning process was implemented on the 'full_session' column. Non-alphabetic characters were filtered out, and the text was converted to lowercase. This cleaning process was crucial for creating a meaningful representation of the session data.\n",
    "\n",
    "In this endeavor, we embark on a journey to construct a robust Session Intent Classification model with the overarching goal of assigning intents to sessions using state-of-the-art techniques, specifically leveraging the Doc2Vec model and TensorFlow.\n",
    "\n",
    "Our odyssey begins with the exploration of a dataset encapsulating session data, with the central task being the classification of intents. We harness the power of the Pandas library to load our dataset from a parquet file, granting us a glimpse into the raw data's structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A critical facet of our journey involves preparing the textual data for model ingestion. Employing a function to clean the text, we filter out non-alphabetic characters and convert the text to lowercase, ensuring uniformity and aiding in subsequent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, we embark on the tokenization journey, a crucial step where we convert the cleaned sessions into tagged documents. This allows us to represent our textual data in a format suitable for training the Doc2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize 'cleaned_session' texts and create tagged documents\n",
    "tagged_data = [TaggedDocument(words=session, tags=[str(i)]) for i, session in enumerate(df_session_cleaned[\"full_session\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building and Training Doc2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"text-align: justify\">{Cleaning the text data by removing non-alphabetic characters and converting text to lowercase. The Gensim library was used to train a Doc2Vec model on the cleaned text data. A vocabulary was buildt and the Doc2Vec model was trained to generate the vector of embeddings for each session text.}</div>\n",
    "\n",
    "Our expedition proceeds with the training of the Doc2Vec model, an unsupervised learning algorithm designed to transform words or documents into numerical vectors. Configuring the model with specific parameters, such as vector size, window size, and epochs, we meticulously craft a representation of the textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Doc2Vec model\n",
    "doc2vec_model = Doc2Vec(vector_size=100, window=5, min_count=1, epochs=20, dm=1)\n",
    "\n",
    "# Build vocabulary\n",
    "doc2vec_model.build_vocab(tagged_data)\n",
    "\n",
    "# Train the model\n",
    "doc2vec_model.train(tagged_data, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "doc2vec_model.save('trained_doc2vec_model.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our grand architecture unfolds as we transition into building and training the classification model, integrating the pre-trained Doc2Vec embeddings.\n",
    "\n",
    "We fashion a Sequential model in TensorFlow, introducing a dense layer to accommodate the Doc2Vec embeddings as the input layer. Subsequently, we append a final dense layer, equipped with softmax activation for multiclass classification.\n",
    "\n",
    "**4.2. Add a last Dense Layer**\n",
    "\n",
    "<div style=\"text-align: justify\">We have trained the Doc2Vec model, which generated embeddings for our text data. Now, to perform classification, we will build a simple Neural Network that will take these embeddings and will add a dense layer for the classification task. The dense layer will have as many neurons as the number of classes we want to predict.</div>\n",
    "\n",
    "Created a neural network model using TensorFlow/Keras. Set up the architecture by adding a Dense layer with the input dimension being the size of the Doc2Vec vectors and a final Dense layer with softmax activation for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the Doc2Vec embeddings as the input layer\n",
    "input_dim = doc2vec_model.vector_size\n",
    "model.add(Dense(input_dim, input_shape=(input_dim,), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the final dense layer for classification\n",
    "num_classes = 7\n",
    "model.add(Dense(num_classes, activation='softmax'))  # Softmax activation for multi-class classification\n",
    "#This code adds the final dense layer for classification on top of the Doc2Vec embeddings. num_classes represents the number of output classes we have in our classification task (in our case, the 7 different intents). The softmax activation function is used here as it's suitable for multi-class classification tasks, providing probabilities for each class.\n",
    "#The purpose of this code is to create a neural network architecture suitable for classification using the Doc2Vec embeddings as input features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">To compile the model, we'll need to set a few parameters:<br>\n",
    "\n",
    "- Optimizer: The optimizer adjusts the weights during training to minimize the loss function.\n",
    "- Loss Function: For multi-class classification, 'categorical_crossentropy' is commonly used.\n",
    "- Metrics: These are used to judge the performance of the model. For classification tasks, 'accuracy' is a standard metric.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#Categorical Crossentropy: This loss function is suitable for multi-class classification problems. It measures the dissimilarity between the true distribution and the predicted distribution of the classes.\n",
    "#Accuracy: It calculates the accuracy of the model, i.e., the number of correctly predicted instances divided by the total number of instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3. Fine-tune the last layer of the network on the supervised training set for N epochs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">Once the dataset was divided into training and validation sets. Converted the data into TensorFlow tensors. Fit the neural network model using the training set, specifying the number of epochs and batch size.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate model training, we convert our tokenized and preprocessed data into TensorFlow tensors. This conversion involves the extraction of vectors and subsequent conversion into arrays, catering to the TensorFlow model's input requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model = Doc2Vec.load('trained_doc2vec_model.model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doc2vec = df_session_cleaned.copy().drop(columns=[\"full_session\", \"first_timestamp\"])\n",
    "df_doc2vec.insert(1, \"doc2vec_vector\", df_session_cleaned['full_session'].apply(doc2vec_model.infer_vector))\n",
    "df_doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME REMOVE THIS PART\n",
    "# Load the trained Doc2Vec model\n",
    "doc2vec_model = Doc2Vec.load('trained_doc2vec_model.model') \n",
    "\n",
    "# Function to infer vectors for each document\n",
    "def infer_vector(text):\n",
    "    return doc2vec_model.infer_vector(text.split())\n",
    "\n",
    "# Apply inference to each text and store the vectors in a new column\n",
    "data['doc2vec_vectors'] = data['full_session'].apply(infer_vector)\n",
    "\n",
    "# Extract vectors and convert to array for modeling\n",
    "X = np.array(data['doc2vec_vectors'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_doc2vec['doc2vec_vector']  \n",
    "y = df_doc2vec['Set_Fingerprint'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Print the shapes \n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "y_train_mlb = mlb.fit_transform(y_train)\n",
    "y_test_mlb = mlb.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_array = np.array(X_train.tolist())\n",
    "X_test_array = np.array(X_test.tolist())\n",
    "\n",
    "X_train_tensor = tf.convert_to_tensor(X_train_array)\n",
    "X_test_tensor = tf.convert_to_tensor(X_test_array)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train_mlb)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test_mlb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent:\n",
    "TensorFlow's optimizer, such as 'adam,' already performs gradient descent during training, so we don't need to explicitly implement gradient descent.\n",
    "\n",
    "To explore the fine-tuning process, the original model was cloned, creating a new model (cloned_model). The cloned model was compiled, and fine-tuning was conducted on the last layer. This step involved training the model on the supervised training set for 10 epochs.\n",
    "\n",
    "The epoch of model training commences as we replicate the architecture and compile the model for training. We embark on an iterative process, honing the model's capacity to discern intents from sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloned_model = clone_model(model)\n",
    "cloned_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training results were analyzed by observing the learning curves over the 10 epochs. The evolution of loss and accuracy on both the training and validation sets provided insights into the model's convergence and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = cloned_model.fit(X_train_tensor, y_train_tensor, epochs=10, batch_size=32, validation_data=(X_test_tensor, y_test_tensor))\n",
    "#Epoch: An epoch represents one complete pass through the entire training dataset.\n",
    "#Number of Epochs: It determines the number of times the learning algorithm will work through the entire training dataset\n",
    "#Batch: The training data is divided into batches. The model is trained on each batch, and the weights are updated after each batch.\n",
    "#Batch Size: It's the number of samples processed before the model is updated. Smaller batch sizes offer faster training but may be less stable than larger batch sizes.\n",
    "#model.fit(): This function fits the model on the training data. It iterates over a fixed number of epochs and updates the model's weights based on the backpropagation of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss and Accuracy Trends:\n",
    "\n",
    "Training Loss:\n",
    "Decreases from an initial value of 189.9254 to 12626.7578 over the 10 epochs.\n",
    "Training Accuracy:\n",
    "Starts at 75.37% and drops to 47.99% by the end of the training.\n",
    "\n",
    "Validation Loss and Accuracy:\n",
    "\n",
    "Validation Loss:\n",
    "Increases from 439.7433 to 13605.3906 during the training process.\n",
    "Validation Accuracy:\n",
    "Exhibits fluctuations, reaching a peak of 91.45% in the final epoch.\n",
    "\n",
    "Overfitting:\n",
    "The model appears to be overfitting, as indicated by the increasing validation loss and decreasing training accuracy. This suggests that the model is fitting the training data too closely\n",
    "\n",
    "Validation Accuracy Fluctuations: \n",
    "The validation accuracy shows fluctuations, indicating potential instability in the model's performance.\n",
    "\n",
    "<div style=\"text-align: justify\">When using <i>'tf.function'</i>, TensorFlow creates a graph representation of the computation for optimization. However, certain operations, especially those involving variable creation, can cause issues within the graph when the function is called multiple times. <br><br>\n",
    "Creating a new model instance or cloning the model before train it again helps to avoid conflicts caused by the pre-existing variables or the TensorFlow graph. This approach prevents potential issues that might arise from reusing the same model object within a <i>'tf.function'</i> context. Essentially, it ensures that the new training session starts with a fresh model instance, avoiding any residual state from previous training sessions that might interfere with the current one.</div><br>\n",
    "\n",
    "**4.4 Plot the learning curves on training and validation set. After how many epochs should we stop the training?**\n",
    "\n",
    "<div style=\"text-align: justify\">The Matplotlib library was used to create visualisations of training and validation losses, as well as training and validation accuracy over epochs. These plots are helpful to understand how the model learns over time, showing convergence and possible overfitting or underfitting.</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">(Learning curves are used to understand the changes in 'loss' and 'accuracy' at each epoch. If the training accuracy is high but the validation accuracy is low, it may indicate overfitting, whereas consistently low accuracy for both may suggest underfitting).</div><br>\n",
    "\n",
    "<div style=\"text-align: justify\">[In summary, the process involved preparing the text data, training a Doc2Vec model to generate embeddings, constructing and training a neural network model for intent classification, and visualizing the model's learning progress. The aim is to build a model that effectively identifies attack session intents using text data and Doc2Vec embeddings.]</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the training history\n",
    "training_loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "training_accuracy = history.history['accuracy']\n",
    "validation_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Set up Seaborn style\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_loss, label='Training Loss')\n",
    "plt.plot(validation_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(training_accuracy, label='Training Accuracy')\n",
    "plt.plot(validation_accuracy, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OTHER POSSIBLE SOLUTION to improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloned_model_improvement = clone_model(model)\n",
    "# Compile the model with a different optimizer and learning rate\n",
    "cloned_model_improvement.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Add dropout layers\n",
    "model.add(Dropout(0.5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement Early Stopping: Monitor the validation loss and stop training when it ceases to improve.\n",
    "# Add early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history = cloned_model_improvement.fit(X_train_tensor, y_train_tensor, epochs=10, batch_size=32, validation_data=(X_test_tensor, y_test_tensor), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "precision, recall, and F1-score for a more comprehensive assessment of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Get predictions\n",
    "predictions = cloned_model_improvement.predict(X_test_tensor)\n",
    "\n",
    "# Convert probabilities to binary predictions using thresholding\n",
    "threshold = 0.5\n",
    "predictions_binary = (predictions > threshold).astype(int)\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(y_test_mlb, predictions_binary, average='weighted')\n",
    "recall = recall_score(y_test_mlb, predictions_binary, average='weighted')\n",
    "f1 = f1_score(y_test_mlb, predictions_binary, average='weighted')\n",
    "\n",
    "print(f'Precision: {precision}, Recall: {recall}, F1-Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<left><b><font size=4>Final Considerations of Section 4<b><left>\n",
    "\n",
    "The initial model displayed promising results with high validation accuracy but raised concerns about potential overfitting due to a significant gap between training and validation accuracy.\n",
    "\n",
    "#### Improvement Steps:\n",
    "\n",
    "- **Model Architecture Adjustment:**\n",
    "  - *Dropout Layers:* Incorporated dropout layers to introduce regularization, mitigating overfitting.\n",
    "  - *Layer Complexity:* Adjusted the model's architecture by fine-tuning layer complexities for improved balance.\n",
    "\n",
    "- **Optimization Strategies:**\n",
    "  - *Learning Rate Tuning:* Experimented with different learning rates to enhance stability during training.\n",
    "  - *Adam Optimizer:* Transitioned to the Adam optimizer for potentially faster convergence.\n",
    "\n",
    "- **Early Stopping Implementation:**\n",
    "  - Introduced early stopping to monitor validation loss, preventing prolonged training that might lead to overfitting.\n",
    "\n",
    "- **Evaluation Metrics Shift:**\n",
    "  - *Weighted Metrics:* Moved from overall accuracy to weighted metrics (precision, recall, and F1-score) for better insight into class-specific performance.\n",
    "\n",
    "  \n",
    "  #### Post-Improvement Model Assessment:\n",
    "\n",
    "- **Training Accuracy (Epochs 1-4):** ~48.03%\n",
    "- **Validation Accuracy (Epochs 1-4):** ~57.06%\n",
    "- **Challenges:** While the overfitting concern has reduced, there's room for improvement in overall accuracy.\n",
    "\n",
    "##### Precision, Recall, and F1-Score Analysis:\n",
    "\n",
    "- **Precision:** Achieving a high precision of *95.85%* indicates the model's ability to correctly classify positive instances.\n",
    "- **Recall:** A recall of *41.45%* suggests the model's capability to capture true positive instances is lower.\n",
    "- **F1-Score:** The F1-score of *53.62%* provides a balanced measure considering both precision and recall.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<left><b><font size=4>Final Considerations of the Project<b><left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
